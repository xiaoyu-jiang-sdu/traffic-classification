# Result
## Experiment Result
###  USTC-TFC2016 (20 cls)
epoch 12

| Model       |  Accuracy  | Precision  |  F1 Score  |   Recall   |
|-------------|:----------:|:----------:|:----------:|:----------:|
| KNN         |   0.9225   |   0.9246   |   0.9213   |   0.9222   |
| 1d-CNN      |   0.9813   |   0.9832   |   0.9813   |   0.9819   |
| 2d-CNN      |   0.9777   |   0.9795   |   0.9775   |   0.9783   |
| LSTM        |   0.9725   |   0.9722   |   0.9721   |   0.9722   |
| **TC-LLM**  | **0.9850** | **0.9849** | **0.9849** | **0.9851** |
| AppScaner   |   0.5822   |   0.7022   |   0.4638   |   0.4941   |
| YaTC        |   0.9829   |   0.9841   |   0.9828   |   0.9829   |
---

### USTC-TFC2016 (2 cls)

| Model      | Accuracy | Precision | F1 Score | Recall |
|------------|:--------:|:---------:|:--------:|:------:|
| **TC-LLM** |  1.0000  |  1.0000   |  1.0000  | 1.0000 |

---

### Tor (8 cls)
epoch 30

| Model          |  Accuracy  | Precision  |  F1 Score  |   Recall   |
|----------------|:----------:|:----------:|:----------:|:----------:|
| KNN            |   0.6050   |   0.6572   |   0.5810   |   0.5740   |
| 1d-CNN         |   0.9732   |   0.9742   |   0.9722   |   0.9713   |
| 2d-CNN         |   0.9789   |   0.9752   |   0.9754   |   0.9757   |
| LSTM           |   0.8873   |   0.9070   |   0.8779   |   0.8668   |
| **TC-LLM**     | **0.9968** | **0.9973** | **0.9973** | **0.9973** |
| AppScaner      |   0.9056   |   0.8885   |   0.8836   |   0.8856   |
| YaTC           |   0.9959   |   0.9960   |   0.9959   |   0.9959   |


## Ablation Study (set up on USTC-TFC2016, epoch 3)
###  different LLM
| Model                         | Accuracy | Precision | F1 Score | Recall |
|-------------------------------|:--------:|:---------:|:--------:|:------:|
| Deepseek R1 Distill Qwen 7B   |  0.9711  |  0.9712   |  0.9699  | 0.9701 |
| Deepseek R1 Distill Qwen 1.5B |  0.9731  |  0.9747   |  0.9729  | 0.9737 |
| Deepseek R1 Distill Llama 8B  |  0.9730  |  0.9732   |  0.9727  | 0.9732 |
| llama-7b                      |  0.9525  |  0.9543   |  0.9517  | 0.9527 |
| longformer                    |  0.4364  |  0.4508   |  0.3928  | 0.4351 |

### different structure

| Model                     | Accuracy | Precision | F1 Score | Recall |
|---------------------------|:--------:|:---------:|:--------:|:------:|
| TC-LLM                    |  0.9711  |  0.9712   |  0.9699  | 0.9701 |
| TC-LLM (plain text embed) |  0.7425  |  0.7840   |  0.7392  | 0.7438 |
| TC-LLM (no reprogramming) |  0.3916  |  0.3981   |  0.3244  | 0.3909 | 

自定义嵌入层+重编程层效果较好。
全部舍弃使用LLM得tokenizer效果下降很大
只使用自定义嵌入层，不进行重编程层效果很差，目前训练1个epoch，损失在2.3附近，难以收敛